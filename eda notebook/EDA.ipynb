{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd989420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pacotes\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Plotar gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import missingno as msno\n",
    "\n",
    "# Combinaçao entre variaveis para plot em EDA\n",
    "from itertools import combinations\n",
    "\n",
    "# trabalhar com arquivo yaml\n",
    "import yaml\n",
    "\n",
    "# Save and load Modelos\n",
    "import joblib\n",
    "\n",
    "# Normalização de Escala\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Redução de dimensionalidade\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# VarianceThreshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Preparar para treinamentos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelos de Predição\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# Métricas/Score de regresssão\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58021a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir parametros de Confif YAML\n",
    "with open(\"./config/conf.yaml\", 'r') as stream:\n",
    "    conf_features = yaml.safe_load(stream)\n",
    "\n",
    "# Definição de variváveis\n",
    "target_prediction = conf_features['target_prediction']\n",
    "iron_concentrate = conf_features['iron_concentrate']\n",
    "\n",
    "try:\n",
    "    colunas = conf_features['colunas']\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4c261",
   "metadata": {},
   "source": [
    "## Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6bec3",
   "metadata": {},
   "source": [
    "### Raw Data\n",
    "##### - Não é preciso processar todas as vezes\n",
    "##### - Se desejar, pode seguir para Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version from KaggleHub\n",
    "path = kagglehub.dataset_download(\"edumagalhaes/quality-prediction-in-a-mining-process\")\n",
    "\n",
    "data = pd.read_csv(Path(path).joinpath(conf_features['rawdata_filename']), sep=',', header=0)\n",
    "\n",
    "# salvar na pasta do projeto\n",
    "data.to_csv('./data/01_raw/' + conf_features['rawdata_filename'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4635a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura de Raw Data\n",
    "data = pd.read_csv('./data/01_raw/' + conf_features['rawdata_filename'], sep=',', header=0)\n",
    "\n",
    "for column in data.columns:\n",
    "    data[column] = data[column].apply(lambda x : x.replace(',' , '.'))\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "data.set_index('date', inplace = True)\n",
    "\n",
    "data = data.astype('double')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ea56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir as colunas do dataset\n",
    "colunas = list(data.columns)\n",
    "\n",
    "conf_features['colunas'] = colunas\n",
    "\n",
    "with open(\"./config/conf.yaml\", 'w') as outfile:\n",
    "    yaml.dump(conf_features, outfile, default_flow_style=False)\n",
    "\n",
    "colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef93b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualizar resumo de informações sobre o dataset\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c972795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustar date timestamp \n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Criação do DataFrame\n",
    "lista_datas = data['date'].unique()\n",
    "for date_time in lista_datas:\n",
    "    size_df = data[data['date'] == date_time]['date'].size\n",
    "    df = pd.DataFrame(pd.date_range(date_time, date_time + timedelta(hours=1), freq=\"20s\", name = 'date'))\n",
    "    data.loc[data['date'] == date_time, 'date'] = (df[-size_df:] - timedelta(seconds=20)).values\n",
    "\n",
    "data.set_index('date', inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000acdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar variável de interesse (target) na linha do tempo\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.scatter(data.index, data[target_prediction])\n",
    "plt.title(target_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79334b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Dataset corrigindo 'date' com timestamp\n",
    "df = pd.DataFrame(pd.date_range(data.index.min(), data.index.max(), freq=\"20s\", name = 'date'))\n",
    "df = pd.concat([df.set_index('date'), data], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efc9ab",
   "metadata": {},
   "source": [
    "### Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb776b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salver Dataset Preprocessado\n",
    "df.to_csv('./data/02_preprocessed/dataset.csv', sep = ';')\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('./data/02_preprocessed/dataset.csv', sep = ';')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização de Missing Data\n",
    "msno.matrix(df, freq = 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar data em periodo que houve operação contínua\n",
    "df = df[df.index> df[df[target_prediction].isna()].index[-1]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salver Dataset Time Filtered\n",
    "df.to_csv('./data/02_preprocessed/dataset_timefiltered.csv', sep = ';')\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('./data/02_preprocessed/dataset_timefiltered.csv', sep = ';')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54918ec9",
   "metadata": {},
   "source": [
    "## Preparação de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1cb5c7",
   "metadata": {},
   "source": [
    "#### - Vizualização do dataset antes de tratamento para Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4054d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar histogramas das variáveis, \n",
    "# Finalidade em oberservar o comportamento de suas distribuição\n",
    "for col in colunas:\n",
    "    fig = px.histogram(df, x = col)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in colunas:\n",
    "  plt.figure(figsize=(18,6))\n",
    "  fig = plt.scatter(df.index, df[col])\n",
    "  plt.title(col)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26193b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizar uma janela de 1 hora para amortecer ruído do dado e do processo \n",
    "window_size = 180\n",
    "\n",
    "for coluna in colunas:\n",
    "\n",
    "    # feature media movel\n",
    "    df.loc[:, coluna] = df[coluna].rolling(window_size).mean()\n",
    "\n",
    "    # feature para observar/filtrar dados com altavariabilidade e transição de operação / operação estável\n",
    "    df[f'{coluna}-std'] = ''\n",
    "    df.loc[:, f'{coluna}-std'] = df[coluna].rolling(window_size).std()\n",
    "\n",
    "df.dropna(inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413acc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salver Dataset de Features\n",
    "df.to_csv('./data/03_features/preliminary_feature_dataset.csv', sep = ';')\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('./data/03_features/preliminary_feature_dataset.csv', sep = ';')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd0559",
   "metadata": {},
   "source": [
    "### Análise das Features\n",
    "#### - Análise Exploratória de Dados (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descrição estatísitca\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad869141",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotar histogramas das variáveis, \n",
    "# Finalidade em oberservar o comportamento de suas distribuição\n",
    "for col in colunas:\n",
    "    fig = px.histogram(df, x = col)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in colunas:\n",
    "  plt.figure(figsize=(18,6))\n",
    "  fig = plt.scatter(df.index, df[col])\n",
    "  plt.title(col)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtra instabilidade no processo pela variabilidade de air flow\n",
    "df = df[df['Flotation Column 01 Air Flow-std']<0.3]\n",
    "df = df[df['Flotation Column 02 Air Flow-std']<0.3]\n",
    "df = df[df['Flotation Column 03 Air Flow-std']<0.3]\n",
    "df = df[df['Flotation Column 04 Air Flow-std']<0.3]\n",
    "df = df[df['Flotation Column 05 Air Flow-std']<0.3]\n",
    "df = df[df['Flotation Column 06 Air Flow-std']<0.3]\n",
    "df = df[df['Flotation Column 07 Air Flow-std']<0.3]\n",
    "\n",
    "# filtra instabilidade no processo pela variabilidade de Level\n",
    "df = df[df['Flotation Column 01 Level-std']<3]\n",
    "df = df[df['Flotation Column 02 Level-std']<3]\n",
    "df = df[df['Flotation Column 03 Level-std']<3]\n",
    "df = df[df['Flotation Column 04 Level-std']<3]\n",
    "df = df[df['Flotation Column 05 Level-std']<3]\n",
    "df = df[df['Flotation Column 06 Level-std']<3]\n",
    "df = df[df['Flotation Column 07 Level-std']<3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d61716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salver Dataset de Features\n",
    "df[colunas].to_csv('./data/03_features/feature_dataset.csv', sep = ';')\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('./data/03_features/feature_dataset.csv', sep = ';')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar histogramas das variáveis, \n",
    "# Finalidade em oberservar o comportamento de suas distribuição\n",
    "for col in colunas:\n",
    "    fig = px.histogram(df, x = col)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1877168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar alguns gráficos para análise de correlação de variávies\n",
    "for gx, gy in [['% Silica Feed', target_prediction],\n",
    "               ['% Iron Feed', '% Iron Concentrate'],\n",
    "               ['% Iron Concentrate', target_prediction],\n",
    "               ['% Iron Feed', '% Silica Feed'],\n",
    "               ['% Silica Feed', '% Iron Concentrate'],\n",
    "              ]:\n",
    "    # plotar grafico realcionando 2 variáveis\n",
    "    fig = plt.figure(figsize = (8,6))\n",
    "    sns.scatterplot(df,\n",
    "                    x = gx, \n",
    "                    y = gy,\n",
    "                    hue = target_prediction,\n",
    "                    palette = 'viridis',\n",
    "                    linewidth=0.1\n",
    "                )\n",
    "    fig.figure.savefig(f\"./EDA Results/Scatter Plot/{gx}x{gy}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93437d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gx, gy in combinations(df[colunas].drop(columns=target_prediction), 2):\n",
    "#     # plotar grafico realcionando 2 variáveis\n",
    "#     fig = plt.figure(figsize = (8,6))\n",
    "#     sns.scatterplot(df,\n",
    "#                     x = gx, \n",
    "#                     y = gy,\n",
    "#                     hue = target_prediction,\n",
    "#                     palette = 'viridis',\n",
    "#                     linewidth=0.1\n",
    "#                 )\n",
    "#     # fig.figure.savefig(f\"./EDA Results/Scatter Plot/{gx}x{gy}.png\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlação entre features\n",
    "plt.figure(figsize=(14,12))\n",
    "plot = sns.heatmap(df[colunas].corr(method = 'pearson'),annot=True,fmt=\".2f\", linewidth=.5)\n",
    "plt.show()\n",
    "plot.figure.savefig(f\"./EDA Results/feature_correlation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb9d12",
   "metadata": {},
   "source": [
    "# Treinamento de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506c92a",
   "metadata": {},
   "source": [
    "## Regressão\n",
    "#### Previsão de % Silica Concentrate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando o dataset para executar treinamento de predição/regressão\n",
    "# Colunas das variáveis utilizadas na prediçao/regressão \n",
    "# (deve avaliar qualidade do modelo sem a variavel % Iron Concentrate, pois este também um resultado do processo)\n",
    "Y = df[target_prediction]\n",
    "X = df[colunas].drop(columns=[target_prediction, iron_concentrate])\n",
    "\n",
    "# Incluir features no arquivo de configuração\n",
    "conf_features['features'] = list(X.columns)\n",
    "with open(\"./config/conf.yaml\", 'w') as outfile:\n",
    "    yaml.dump(conf_features, outfile, default_flow_style=False)\n",
    "\n",
    "# selecionar apenas as features desejadas para o treinmaento\n",
    "X = X[conf_features['features']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizaçao de nomes dos modelos testados\n",
    "names = [\n",
    "    \"Linear\",\n",
    "    \"Ridge\",\n",
    "    \"LARS Lasso\",\n",
    "    # \"SRV\", # SRV não apresentou treinamento satisfatorio, ainda com longo tempo de treinamento\n",
    "    \"RandomForestRegressor\",\n",
    "    \"GradientBoostingRegressor\",\n",
    "    \"AdaBoostRegressor\"\n",
    "]\n",
    "\n",
    "# lista dos modelos testados\n",
    "regression_models = [\n",
    "    linear_model.LinearRegression(),\n",
    "    linear_model.Ridge(alpha=.5),\n",
    "    linear_model.LassoLars(alpha=.1),\n",
    "    # svm.SVR(),\n",
    "    RandomForestRegressor(max_depth=10, random_state=42),\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    AdaBoostRegressor(random_state=42, n_estimators=100)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945545ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrica para valiar razao entre F1-score\n",
    "f1score_ration = pd.DataFrame([], columns=names)\n",
    "\n",
    "# Iteração entre cada modelo\n",
    "for name, reg_model in zip(names, regression_models):\n",
    "\n",
    "    # # MinMaxScaler\n",
    "    # # Possibilidade de trabalhar com ajustando dimensão e escala\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # # Variance Threshold\n",
    "    # # Possibilidade de trabalhar com VarianceThreshold\n",
    "    variance_selector = VarianceThreshold()\n",
    "\n",
    "    # # PCA\n",
    "    # # Possibilidade de trabalhar com reduçao de dimensionaldiade\n",
    "    pca = PCA(n_components=10)\n",
    "    \n",
    "    # criando pipeline com modelo e feature selection\n",
    "    pipeline = Pipeline([\n",
    "        # ('scaler', scaler),\n",
    "        # ('variance', variance_selector),\n",
    "        # ('pca', pca),\n",
    "        ('regression_model', reg_model)\n",
    "    ])\n",
    "\n",
    "    # realizando o treinmaneto da classificação\n",
    "    reg = pipeline.fit(X_train, y_train)\n",
    "    joblib.dump(reg, f'./models/prediction_{name}.joblib')\n",
    "\n",
    "    # capitura de score nos dados de teste\n",
    "    y_pred_train = reg.predict(X_train)\n",
    "    y_pred_test = reg.predict(X_test)\n",
    "    score_train = reg.score(X_train, y_train)\n",
    "    score_test = reg.score(X_test, y_test)\n",
    "    \n",
    "    # printar os resultados do modelo de classificação\n",
    "    print(f'======================={name}==================================')\n",
    "    print('Train')\n",
    "    print('Mean Absolute Error (MAE) = ', metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "    print('Mean Squared Error (MSE) = ',  metrics.mean_squared_error(y_train, y_pred_train))\n",
    "    print('Root Mean Squared Error (RMSE) = ', metrics.root_mean_squared_error(y_train, y_pred_train))\n",
    "    print('R-squared (R2) Score = ', score_train)\n",
    "    print('')\n",
    "    print('Test')\n",
    "    print('Mean Absolute Error (MAE) = ', metrics.mean_absolute_error(y_test, y_pred_test))\n",
    "    print('Mean Squared Error (MSE) = ',  metrics.mean_squared_error(y_test, y_pred_test))\n",
    "    print('Root Mean Squared Error (RMSE) = ', metrics.root_mean_squared_error(y_test, y_pred_test))\n",
    "    print('R-squared (R2) Score = ', score_test)\n",
    "    # residuos apresentão distribuição normal - teste de Shapiro-Wilk\n",
    "    display(stats.shapiro(y_test - y_pred_test))\n",
    "    display(stats.anderson(y_test - y_pred_test))\n",
    "\n",
    "    plt.figure(figsize = (8,6))\n",
    "    sns.histplot(y_test - y_pred_test).set_title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbe2d6",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "#### - Modelo Escolhido é Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de model\n",
    "reg = joblib.load('./models/prediction_RandomForestRegressor.joblib')\n",
    "y_pred_test = reg.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame({'Valor Verdadeiro': y_test,\n",
    "                        'Valor Predito': reg.predict(X_test)})\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.regplot(df_plot,\n",
    "                x = 'Valor Verdadeiro',\n",
    "                y = 'Valor Predito',\n",
    "                scatter_kws=dict(alpha=0.15),\n",
    "                line_kws=dict(color=\"r\", alpha=0.4)\n",
    "                ).set_title('Random Forest Teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66683efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'feature': X.columns,\n",
    "                                   'importance': reg.named_steps['regression_model'].feature_importances_})\n",
    "most_import_feature = feature_importance.sort_values(by='importance', ascending=False)\n",
    "most_import_feature[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se deseja reduzir features de maior importancia no arquivo de configuração\n",
    "# conf_features['features'] = list(most_import_feature[:9]['feature'])\n",
    "# with open(\"./config/conf.yaml\", 'w') as outfile:\n",
    "#     yaml.dump(conf_features, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975cf044",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "sns.barplot(feature_importance.sort_values(by='importance', ascending=False),\n",
    "            x = 'importance', \n",
    "            y = 'feature',\n",
    "            hue = 'importance',\n",
    "            palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e0d59",
   "metadata": {},
   "source": [
    "# Analise de Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cenario 1 \n",
    "## Análise de Clusters com qualiadde de entrara e sáida do processo\n",
    "X_cluster = ['% Iron Feed',\n",
    "             '% Silica Feed',\n",
    "             '% Iron Concentrate',\n",
    "             '% Silica Concentrate']\n",
    "\n",
    "data_cluster = df[X_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cenario 2\n",
    "## Análise de Clusters com dados de variaveis mais importantes da regressão\n",
    "X_cluster = list(most_import_feature[:9]['feature'])\n",
    "X_cluster.append(iron_concentrate)\n",
    "X_cluster.append(target_prediction)\n",
    "\n",
    "data_cluster = df[X_cluster]\n",
    "\n",
    "X_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "k_cand = [2,3,4,5,6,7]\n",
    "\n",
    "for k in k_cand:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(data_cluster)\n",
    "    score0 = kmeans.inertia_\n",
    "    score1 = silhouette_score(data_cluster, kmeans.labels_, metric='euclidean')\n",
    "    score2 = silhouette_score(data_cluster, kmeans.labels_, metric='correlation')\n",
    "    results[k] = {'k':kmeans, \n",
    "                  's0':score0, \n",
    "                  's1':score1, \n",
    "                  's2':score2}\n",
    "    display(results[k])\n",
    "\n",
    "fig,axs = plt.subplots(1,2,sharex=True,figsize=(10,3))\n",
    "axs[0].plot([i for i in results.keys()], [i['s0'] for i in results.values()], 'o-', label='Inertia')\n",
    "axs[1].plot([i for i in results.keys()], [i['s1'] for i in results.values()], 'o-', label='Euclidean')\n",
    "axs[1].plot([i for i in results.keys()], [i['s2'] for i in results.values()], 'o-', label='Correlation')\n",
    "for ax in axs:\n",
    "    ax.set_xticks(k_cand)\n",
    "    ax.set_xlabel('K')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f43ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = kmeans\n",
    "df2 = pd.DataFrame(clustering.labels_)\n",
    "df2 = df2.rename(columns = {0: 'valor'})\n",
    "df2['cont'] = 1\n",
    "df2.groupby(by = ['valor']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfe818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpectralClustering não performou, dataset muito grande\n",
    "# calculating the distance between every vector in your dataset and every other vector\n",
    "# clustering = SpectralClustering(n_clusters=4,\n",
    "#         assign_labels='discretize',\n",
    "#         random_state=0).fit(data_cluster)\n",
    "\n",
    "# clustering = DBSCAN(eps=100, min_samples=20).fit(data_cluster)\n",
    "\n",
    "# clustering = HDBSCAN(copy=True, min_cluster_size=20).fit(data_cluster)\n",
    "\n",
    "clustering = OPTICS(min_samples=20).fit(data_cluster)\n",
    "\n",
    "# clustering = AgglomerativeClustering(n_clusters=20).fit(data_cluster)\n",
    "\n",
    "# análise dos clusters\n",
    "df2 = pd.DataFrame(clustering.labels_)\n",
    "df2 = df2.rename(columns = {0: 'valor'})\n",
    "df2['cont'] = 1\n",
    "df2.groupby(by = ['valor']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbca454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cluster.drop(columns =['level_0', 'index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cluster[target_prediction] = df[target_prediction]\n",
    "lista = data_cluster.columns\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster.loc[:,'group'] = clustering.labels_.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21daa4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = data_cluster.reset_index().melt(id_vars=['date','group'], var_name='Variavel', value_name='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data_cluster\n",
    "\n",
    "data_cluster.reset_index(inplace=True)\n",
    "\n",
    "data_cluster['date'] = pd.to_datetime(data_cluster['date'])\n",
    "\n",
    "# data2.reset_index(inplace = True)\n",
    "\n",
    "data2.loc[:,'month'] = data2['date'].dt.year.astype(str) + '/' + data2['date'].dt.month.map(\"{:02}\".format).astype(str)\n",
    "data2.loc[:,'day'] = data2['date'].dt.day\n",
    "\n",
    "data2.loc[:,'date'] = pd.to_datetime(data2['date']).dt.to_period('M').astype(str)\n",
    "data2.loc[:,'count'] = 1\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af428456",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(data2[['date', 'month', 'group', 'count']].groupby(['date', 'group']).sum().reset_index().sort_values(by = 'group'),\n",
    "      x = 'date',\n",
    "      y = 'count',\n",
    "      color = 'group',\n",
    "      barmode = 'group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar box plot, entre mêses\n",
    "for col in lista:\n",
    "    fig = px.box(data_cluster.sort_values(by = 'group'),\n",
    "           x = 'group',\n",
    "           y = col,\n",
    "           color = 'group'\n",
    "                )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0cbe1",
   "metadata": {},
   "source": [
    "## Analise de Monte Carlo\n",
    "### - Simular ceários de operação, de tal forma a entender as tomasdas de decisões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2258a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(most_import_feature['feature'])\n",
    "X.append(target_prediction)\n",
    "\n",
    "data_simulation = df[X]\n",
    "data_simulation.loc[:, 'group'] = clustering.labels_.astype(str)\n",
    "data_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c41d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_from = '6'\n",
    "grupo = data_simulation[data_simulation['group'] == cluster_from ]\n",
    "\n",
    "cluster_to = '2'\n",
    "parameters_toadjust = ['Amina Flow',\n",
    "                      'Flotation Column 04 Air Flow',\n",
    "                      'Flotation Column 02 Level',\n",
    "                      'Flotation Column 06 Level',\n",
    "                      'Flotation Column 07 Level'\n",
    "                      ]\n",
    "grupo_adjust = data_simulation[data_simulation['group'] == cluster_to ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024711b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(most_import_feature.sort_index()['feature'])\n",
    "Y = target_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = { col: np.random.uniform(grupo.describe()[col]['25%'], grupo.describe()[col]['75%'], 1000) for col in X}\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carlo = pd.DataFrame(dic)\n",
    "df_carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carlo_adjusted = df_carlo.copy()\n",
    "\n",
    "for col in parameters_toadjust:\n",
    "    df_carlo_adjusted.loc[:,col] = np.random.uniform(grupo_adjust.describe()[col]['25%'], grupo_adjust.describe()[col]['75%'], 1000)\n",
    "\n",
    "df_carlo_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4624d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_data = pd.DataFrame([], columns = ['Distribuição', 'Valores'])\n",
    "plot_y_data['Valores'] = df[Y]\n",
    "plot_y_data['Distribuição'] = 'Real Global'\n",
    "plot_y_data\n",
    "\n",
    "aux = pd.DataFrame([], columns = ['Distribuição', 'Valores'])\n",
    "aux['Valores'] = grupo[Y]\n",
    "aux['Distribuição'] = f'Real Grupo {cluster_from}'\n",
    "\n",
    "plot_y_data = pd.concat([plot_y_data, aux])\n",
    "plot_y_data\n",
    "\n",
    "\n",
    "aux = pd.DataFrame([], columns = ['Distribuição', 'Valores'])\n",
    "aux['Valores'] = reg.predict(df_carlo[X])\n",
    "aux['Distribuição'] = 'Monte Carlo Random Forest'\n",
    "\n",
    "plot_y_data = pd.concat([plot_y_data, aux])\n",
    "plot_y_data\n",
    "\n",
    "\n",
    "aux = pd.DataFrame([], columns = ['Distribuição', 'Valores'])\n",
    "aux['Valores'] = reg.predict(df_carlo_adjusted[X])\n",
    "aux['Distribuição'] = 'Random Forest - Parâmetros Ajustados'\n",
    "\n",
    "plot_y_data = pd.concat([plot_y_data, aux])\n",
    "plot_y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(\n",
    "    plot_y_data, \n",
    "    x = 'Distribuição',\n",
    "    y = 'Valores',\n",
    "    color = 'Distribuição'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grupo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carlo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carlo_adjusted.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d860ad8",
   "metadata": {},
   "source": [
    "# Fim !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
